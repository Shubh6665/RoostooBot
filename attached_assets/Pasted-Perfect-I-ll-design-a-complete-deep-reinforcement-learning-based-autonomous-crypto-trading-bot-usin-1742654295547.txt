Perfect. I’ll design a complete deep reinforcement learning-based autonomous crypto trading bot using the Roostoo mock exchange API. It will use the most suitable RL model (like SAC or PPO), trade high-volatility coins for maximum 24-hour return, and include advanced techniques to maximize Sharpe Ratio with live execution support.
I’ll now begin gathering the best strategies, frameworks, and architecture for building the bot, including coin selection, training setup, optimal trading model, and live deployment.
I’ll get back to you shortly with a complete solution and codebase overview.
Deep Reinforcement Learning Crypto Trading Bot for Roostoo
1. Algorithm Selection for Short-Term Trading
Short-term crypto trading requires an RL algorithm that can handle continuous decision-making and noisy, fast-changing data. Popular choices include Deep Q-Networks (DQN) and policy-gradient methods like Proximal Policy Optimization (PPO) or Soft Actor-Critic (SAC). DQN (and its variants like Double DQN or Dueling DQN) has been applied to crypto trading (Cryptocurrency Trading Agent Using Deep Reinforcement Learning), but it works on discrete action spaces (e.g. fixed buy/sell signals) and may need extensive tuning for stability. In contrast, policy-gradient algorithms (like PPO) are often preferred for trading because they naturally handle continuous or multi-dimensional actions and have robust training performance (Building a trading bot with Deep Reinforcement Learning(DRL) | by Zhi Li | DataPebbles | Medium). PPO in particular is known for stable learning and has been highlighted by OpenAI and many research papers as a reliable choice for financial RL (Building a trading bot with Deep Reinforcement Learning(DRL) | by Zhi Li | DataPebbles | Medium). SAC is another strong candidate for continuous action spaces, offering sample-efficient learning and good exploration, but it can be more complex to tune. Given the need for quick learning and decisive actions in a 24-hour window, PPO strikes a good balance of stability and performance for our bot. We will use PPO (from Stable Baselines3) as the core algorithm, which can output continuous action values or probabilities for discrete actions.
2. Selecting High-Volatility Cryptocurrencies
To maximize 24h returns, the bot should trade assets that exhibit significant intraday price swings. The Roostoo mock exchange offers various trading pairs; we choose one or more with historically high volatility and liquidity. Generally, smaller-cap or hype-driven coins (often “meme” coins or newer tokens) show the most extreme swings (Top 10 Most Volatile Crypto Coins to Trade in 2025), but large-cap coins like Bitcoin (BTC) and Ethereum (ETH) also experience substantial volatility and high trading volume. In fact, even top assets like BTC/ETH can move several percent within hours, providing ample opportunity for short-term trades (Top 10 Most Volatile Crypto Coins to Trade in 2025). For this project, we’ll focus on BTC/USD (or BTC/USDT) as a baseline (it’s widely traded and volatile), and optionally a second coin such as Binance Coin (BNB) which Roostoo supports (e.g. BNB/USD) and is known for significant intraday moves. These choices ensure the agent has enough price movement to learn profitable patterns. We will avoid low-volume or stablecoins since their lack of movement is not suitable for a trading strategy. By selecting highly active markets, the bot can capitalize on rapid price fluctuations within each 24-hour cycle.
3. Training Environment with Historical Market Data
Before live trading, we train the RL agent on a simulated market environment using historical data. This environment will emulate real market dynamics so the agent can learn patterns and strategies safely. We obtain historical price data from sources like Kaggle or Binance for the chosen cryptocurrencies. For example, Kaggle provides datasets of OHLC (open-high-low-close) price data for major cryptos over years (Cryptocurrency Price Analysis Dataset - Kaggle). Using such data, we construct a custom OpenAI Gym environment (TradingEnv) that feeds the agent with observations and computes rewards.
State (Observation): Each observation represents the current market state and portfolio status. We include features such as recent prices, technical indicators, and current holdings. For simplicity, the state can start with just the latest price (and maybe a short window of previous prices or moving averages) along with the agent’s current position (e.g. holding or not holding the asset). It's important to normalize features (e.g. price changes or technical indicators) to a range like [-1, 1] for stable learning (Building a trading bot with Deep Reinforcement Learning(DRL) | by Zhi Li | DataPebbles | Medium). We could expand the observation to include indicators like a short-term and long-term moving average or volatility metrics to help the agent recognize trends. The environment maintains an internal memory of price history to compute these features.
Action Space: We define a discrete action space with three actions – 0: HOLD, 1: BUY, 2: SELL (Building a trading bot with Deep Reinforcement Learning(DRL) | by Zhi Li | DataPebbles | Medium). This keeps the decision simple and interpretable: the agent can choose to buy a certain amount, sell (if holding), or do nothing on each step. (We assume no short-selling for simplicity, so the agent will only sell if it currently holds the asset.) Each BUY or SELL action will typically be for a fixed position size or a fraction of available balance – we can start with a fixed size (e.g. buy with 100% of cash or sell all holdings) to simplify. Alternatively, a continuous action space could be used (where the action value might indicate what fraction of the portfolio to allocate to the asset), but a discrete approach combined with a risk management module for position sizing is easier to implement and sufficient for demonstrating the strategy.
State Transitions: The environment steps through time using historical price data. At each time step (e.g. each minute or each price tick in the dataset), it provides the current price and portfolio state. The agent chooses an action, and the environment simulates the result:
* If the agent buys, the environment will decrease the cash balance and increase the crypto holding based on the price.
* If the agent sells, it will convert holdings back to cash.
* If hold, no portfolio change occurs. The next state's price will be the next timestamp’s price from the dataset. This sequential simulation gives the agent feedback on its action’s outcome.
Reward Function: We design the reward to encourage portfolio growth while considering risk. A common reward is the change in portfolio value after each action. For example, reward could be the percentage return of the portfolio over the last step. If the portfolio value grows (after a successful trade or favorable price move), the reward is positive; if it shrinks, reward is negative. We can also incorporate transaction costs or a small penalty for holding risk to make the agent consider trade-offs. Another refinement is to shape the reward toward achieving a high Sharpe Ratio: e.g. penalize large volatility in returns or losses more than gains. However, for simplicity, we start with net profit as reward, letting the agent implicitly learn risk management. By the end of an episode (e.g. one day of data), the cumulative reward reflects the total profit for that period.
Episode Termination: We can treat each 24-hour cycle as one episode (especially if we want the agent to maximize 24h return). In training, an episode could span the length of the historical dataset (which might be many days) or we can explicitly cut episodes into 24h segments to align with the evaluation metric. For training efficiency, using the entire dataset as one long episode is fine, but we might also randomize start points or shuffle through multiple days to expose the agent to varied conditions.
With this environment in place, we essentially have a framework where the agent observes state, takes an action, and receives a reward, mimicking how it will interact with the real exchange. This kind of gym-style interface is standard for training trading agents (Building a trading bot with Deep Reinforcement Learning(DRL) | by Zhi Li | DataPebbles | Medium). We ensure the environment is stateful: it carries over the portfolio and any open positions from step to step, which reflects real trading where decisions impact future opportunities.
(GitHub - AI4Finance-Foundation/FinRL: FinRL: Financial Reinforcement Learning. ) Conceptual architecture of a DRL-based trading system. The agent interacts with a market environment by observing state and executing actions, then receives rewards based on portfolio performance. We train the agent on historical data (bottom layer) and then deploy it for live trading (top application) (GitHub - roostoo/Roostoo-API-Documents) (GitHub - roostoo/Roostoo-API-Documents).
4. Model Architecture and Hyperparameter Optimization
Our trading agent uses a deep neural network as its policy (and value function estimator, in the case of PPO). We select a Multi-Layer Perceptron (MLP) architecture for the policy network, since the input features are vectorized numerical data (prices, indicators, etc.). A reasonable architecture might be 2-3 hidden layers with 64 to 128 neurons each, using ReLU activations. This has enough capacity to capture non-linear patterns in price movements without being too large (which could overfit or slow down training). We initialize the model with Stable Baselines3’s built-in MlpPolicy for PPO, which by default uses two hidden layers of size 64 – we can adjust this via policy_kwargs if needed. For example, to make decisions more sharply, we might increase layer sizes or add an extra layer to better discriminate subtle market signals, but in practice the default often works well for a start.
Hyperparameters are crucial for fast convergence. We choose:
* Learning Rate: Start around 3e-4 (0.0003) for PPO (Deep Reinforcement Learning for Crypto Trading | by Alex K | Coinmonks | Medium). This is a typical value balancing steady progress with not overshooting optima. We can reduce it if the training seems unstable or increase for faster learning if things are smooth.
* Discount Factor (gamma): Use γ = 0.99, meaning the agent values rewards up to a 24h horizon fairly strongly. A high gamma encourages the agent to maximize longer-term portfolio growth rather than greedy short gains (Deep Reinforcement Learning for Crypto Trading | by Alex K | Coinmonks | Medium).
* PPO-specific: Clip range (epsilon) ~0.2, few epochs per update (e.g. 10), and an adequate batch size. These control how drastic each policy update is and prevent instability (Deep Reinforcement Learning for Crypto Trading | by Alex K | Coinmonks | Medium). We keep the clipping to standard 0.1-0.2 range to ensure updates are not too large.
* Exploration: PPO uses an entropy bonus to encourage exploration. We can start with the default entropy coefficient and adjust if needed (higher entropy coeff if the agent gets stuck in a local strategy too early, or lower it to make the policy more deterministic once it’s performing well (Deep Reinforcement Learning for Crypto Trading | by Alex K | Coinmonks | Medium)). Sharp decision boundaries usually emerge when exploration decays and the policy focuses on exploitation, so we might decrease entropy over time.
* Reward normalization: Enable reward normalization or running norm (Stable Baselines can do this) to stabilize advantage estimates, especially because profit rewards can vary in scale with price.
* Training Timesteps: Because we want quick convergence, we’ll train for a moderate number of timesteps (e.g. 100k to 1M time steps). We monitor performance on a validation set of historical data to avoid overfitting or to early-stop once returns plateau.
During training, we continuously evaluate the agent’s performance on unseen historical segments or via cross-validation (e.g. train on past data, validate on a more recent slice). The goal is to see the agent’s 24h profit and Sharpe improving in simulation. Early stopping can be used if the model stops improving or starts overfitting (e.g. if performance on training data goes up but validation goes down).
Below is a simplified example of training the PPO agent on our custom environment:
import gym
from stable_baselines3 import PPO

# Initialize custom trading environment with historical data
env = TradingEnv(historical_prices, initial_balance=10000)  # user-defined env
env = gym.wrappers.TimeLimit(env, max_episode_steps=24*60)  # limit to 24h steps if needed

# Configure and create PPO model
model = PPO("MlpPolicy", env, learning_rate=3e-4, gamma=0.99, verbose=1,
            tensorboard_log="./tensorboard_logs/")  # enable logging for monitoring

# Train the model
model.learn(total_timesteps=200000)  # train for given timesteps

# Save the trained model
model.save("models/ppo_trading_bot")
During training, hyperparameters like learning rate or batch size can be tuned by observing the reward curve. If convergence is slow, we might increase learning rate or reduce complexity; if the agent is too inconsistent, we ensure the reward signals and features are scaled properly. The result of training is a policy (saved as ppo_trading_bot.zip) that we can load for live trading.
5. Live Trading Pipeline Implementation
Once the model is trained, we deploy it to live trading on Roostoo’s mock exchange. The live trading script will run an infinite loop (or a scheduled loop) that continuously interacts with the exchange API and our model. The main components of this loop are:
1. Fetch Real-Time Market Data: Use Roostoo’s public ticker endpoint to get the latest price quote for the chosen cryptocurrency pair. We call the Roostoo API GET /v3/ticker with the pair symbol and a timestamp (for security) as query parameters. This endpoint does not require a signature (only a valid timestamp and perhaps API key if needed) and returns a JSON with the current price and other stats. For example, for BTC/USD we request .../v3/ticker?pair=BTC/USD&timestamp=<ms>. The API client from our earlier setup handles adding the timestamp and sending the request. We parse the response to extract the last traded price (and possibly bid/ask if needed). The data is then formatted into the state representation expected by our model – for instance, we update our state array with this latest price, and include our current portfolio holdings (from the last step). This real-time data fetch happens at a certain frequency (e.g. every minute or every few seconds) depending on how frequently we want the bot to trade.
2. Generate Model Prediction (Action): With the latest state prepared, we feed it into the trained RL model to get a suggested action. In code, we would do something like: obs = get_current_obs(price, portfolio)  # construct observation vector
3. action, _states = model.predict(obs, deterministic=True)
4.  We use deterministic=True to use the greedy action (since during live trading we want the best action, not a random exploratory one). The output action might be an integer (0,1,2) in our discrete setup. We then interpret this action as HOLD, BUY, or SELL accordingly. If the model outputs a continuous value (in a different design), we’d interpret its magnitude as a position size or probability – but here the discrete action simplifies decision-making.
5. Apply Decision Thresholds: We incorporate a layer of logic to avoid frivolous trades and manage uncertainty. For example, if the model’s recommended action is BUY but the price hasn’t moved significantly or the model’s “confidence” is low, we might choose not to trade (i.e., treat it as HOLD). How to measure confidence? In a discrete policy, we can look at the probability the policy assigns to the chosen action or the difference in Q-values (for DQN). If that confidence is below a threshold (say less than 0.6 probability), the bot could abstain from trading this round. Similarly, we could require a minimum expected return (perhaps estimated from recent price trend) before acting. This step is essentially a safety check to prevent over-trading on noise. By setting thresholds, we ensure the bot has sharp decision boundaries – it only executes a BUY/SELL when the signal is strong enough, otherwise it waits. These thresholds can be tuned based on experience (for instance, only buy if the model’s action probability > 0.7 or if the price change in last 5 minutes exceeds a small percent indicating momentum).
6. Execute Trade via Roostoo API: If the decision is to BUY or SELL, the bot sends an order to Roostoo using the /v3/place_order endpoint. This is a private (signed) endpoint requiring authentication. We construct the order parameters as per Roostoo’s API:
    * pair: the trading pair, e.g. "BTC/USD".
    * side: "BUY" or "SELL".
    * type: order type, we use "MARKET" for immediate execution at market price.
    * quantity: how much to buy or sell (for example, 0.01 BTC). Our position sizing logic (see Risk Management) will determine this quantity.
    * timestamp: current time in ms (to pass the security check).
7. These parameters must be properly signed with our API secret key. Roostoo uses HMAC SHA256 signing (GitHub - roostoo/Roostoo-API-Documents). We create a query string of the params (sorted by key) like "pair=BTC/USD&quantity=0.01&side=BUY&timestamp=163...&type=MARKET", then compute the HMAC SHA256 hash using our secret key (GitHub - roostoo/Roostoo-API-Documents). The resulting hex digest is the MSG-SIGNATURE. In the HTTP headers of the POST request, we include:
    * Content-Type: application/x-www-form-urlencoded (as required for POST) (GitHub - roostoo/Roostoo-API-Documents),
    * RST-API-KEY: <your_api_key> (the public API key ID),
    * MSG-SIGNATURE: <signature> (the HMAC signature we just computed).
8. The final step is to POST the order. For example, using Python requests: url = base_url + "/v3/place_order"
9. response = requests.post(url, data=order_params, headers=headers)
10. result = response.json()
11.  We check result to see if the order was accepted. It should contain details of the executed trade (price, quantity, etc.) or any error message. On success, we log the trade (details in logging section) and update our internal state to reflect the new position (e.g. if we bought, we now have crypto holdings and less cash).
12. Fetch Updated Balance and Portfolio: After an order, or even if we held, we query the current portfolio balances. We can use Roostoo’s balance endpoint (GET /v3/balance) which requires a signed request as well (with just timestamp and our keys) (GitHub - roostoo/Roostoo-API-Documents). The balance data tells us how much USD and BTC (or other asset) we now have. We update the bot’s internal record of balance and crypto_holdings. This is critical for the next decision step – the observation needs to include the latest portfolio info. Alternatively, we can update the portfolio based on the executed order (e.g., if we know we bought 0.01 BTC, just decrement our USD and increment BTC accordingly). But using the balance API is a good practice to double-check and to sync with any external changes. Real-time portfolio monitoring also allows us to compute PnL and ensure we haven’t hit any risk limits.
13. Loop Continuously: The above steps repeat continuously, effectively making the bot trade autonomously. We might run this loop with a sleep interval to control frequency. For example, a loop that runs every 60 seconds is suitable for a strategy based on minute-level data. If we want higher frequency (seconds-level), we must ensure the API and network can handle it and the model can compute quickly. We also include a mechanism to gracefully stop the loop (for instance, listening for a keyboard interrupt or a certain condition like end of 24h period if we want to reset daily).
Throughout this pipeline, robust error handling is important. The bot should catch API errors or network timeouts and retry gracefully. It should also handle scenarios like trying to sell when there’s no balance, or buy when not enough cash (our logic should prevent these, but just in case). By implementing these steps, we bridge the trained model with the real (mock) market in real-time.
6. Risk Management Strategies
Implementing advanced risk management is crucial to protect the portfolio, especially given crypto volatility. We integrate several layers of risk control into the bot:
* Stop-Loss Orders: For every position taken, the bot can set a stop-loss threshold. For example, if we buy BTC at $50,000, we might set a stop-loss at 5% below that (i.e. $47,500). If the market price hits that level, the bot will automatically execute a sell to prevent further loss. Roostoo’s API might allow setting a stop-loss order natively; if not, the bot can monitor prices and trigger a market sell when the threshold is breached. Stop-loss limits help protect against sudden drops (Top 10 Most Volatile Crypto Coins to Trade in 2025), acting as a safety net. We will tune the stop-loss percentage according to the coin’s volatility (a very volatile coin might need a wider stop to avoid getting stopped out on normal fluctuations).
* Take-Profit Targets: Similarly, the bot can secure profits by selling once a trade has made a certain gain. For instance, if we aim for 3% profit on a trade, the bot can sell when that target is reached. This ensures we lock in gains and avoid turning a winning trade into a loss if the market reverses. Take-profits can be set in tandem with stop-loss (this is basically bracket orders). While our RL agent theoretically should learn when to sell, having a take-profit as a hard rule adds an extra layer of protection and can improve Sharpe by capping the variance of outcomes.
* Dynamic Position Sizing: Rather than always trading a fixed amount, the bot adjusts how much to buy/sell based on current risk. A common approach is to risk a fixed percentage of the capital on each trade (position size determined by stop-loss distance). For example, if we allow 1% of capital risk per trade and our stop-loss is 5%, we size the position such that 5% drop would equal 1% of portfolio loss. This results in smaller positions in more volatile conditions. The bot can also reduce trade size after a losing streak to cut risk, and slightly increase after wins (within limits) when confidence is higher. This dynamic sizing helps stabilize the equity curve.
* Max Drawdown Control: We monitor the portfolio’s peak value and current drawdown. If drawdown (percentage drop from the highest portfolio value seen) exceeds a certain threshold (say 10% in a day), the bot can take defensive actions. For example, it might halt trading for a cooldown period or switch to a “safe mode” where it only does very small trades or only exits positions. This prevents a cascade of losses. Essentially, if the bot is down significantly, it stops trading aggressively to “stay in the game” (Top 10 Most Volatile Crypto Coins to Trade in 2025). This kind of circuit-breaker ensures survival during bad market conditions or if the strategy starts failing.
* Diversification (Optional): While our current design may trade one or two assets, risk can be spread out by trading a basket of uncorrelated cryptos. The bot could split capital across multiple coins so that a crash in one coin doesn’t wipe out everything. However, this complicates the state and action space for the RL agent (it becomes a multi-asset portfolio allocation problem). As an intermediate step, we might run multiple instances of the bot, each on a different coin, to achieve some diversification without entangling the learning process.
These risk management rules work alongside the RL model’s decisions. The RL agent decides when to buy or sell based on patterns it learned, but the risk management decides how much to trade and ensures losses are limited. By combining an intelligent agent with strict risk controls, we aim to maximize the Sharpe Ratio (risk-adjusted return), not just raw returns. It’s well known that in volatile markets, using stop-loss orders and other protections is essential (Top 10 Most Volatile Crypto Coins to Trade in 2025). Our bot will log and enforce these rules on every trade decision.
7. Logging and Performance Monitoring
Continuous logging is implemented to track the bot’s behavior and performance metrics. Every action the bot takes (or even decisions to hold) will be recorded with details:
* Trade Logs: Each trade (buy/sell) entry includes timestamp, action type, price, quantity, and resulting portfolio balance. We also log any additional context, such as the model’s predicted action probabilities or value function (to gauge confidence), and any indicators (e.g. “stop-loss hit” or “take-profit hit” flags). Logging these details is invaluable for debugging and for post-mortem analysis of strategy performance.
* Portfolio Value Tracking: The bot keeps a time series of portfolio value (in USD). After each step or each trade, we record the total value of assets (cash + market value of holdings). This allows us to plot an equity curve over time and compute performance statistics.
* Metrics Calculation: Key performance metrics include total PnL (profit and loss) over the 24h period and the Sharpe Ratio of the returns. The Sharpe Ratio is a measure of risk-adjusted return, defined as the average excess return (portfolio return minus risk-free rate) divided by the standard deviation of returns (AI Web3 Trading Bot Backtesting.pdf). Since the trading horizon is short, we can assume risk-free rate ~0 for 24h, or use a very small number like 0.01% for completeness. We compute this by taking the log returns or percentage returns of the portfolio value each time step or each trade, then calculating Sharpe = mean(returns) / std(returns) (annualized as needed, but for intra-day we can just use the 24h window). A high Sharpe indicates good risk-adjusted performance (the bot is earning steady returns with low volatility of outcomes) (Reinforcement Learning-Based Multimodal Model for the Stock ...).
* Console and File Logging: We use Python’s logging module to print info to console and save to a file (e.g. logs/trades.log). For each iteration, we might output something like: 2025-03-22 10:05:00 - BUY 0.01 BTC at 50000 USD, Portfolio value: 10,500 USD, Confidence: 0.8 2025-03-22 10:30:00 - SELL 0.01 BTC at 51000 USD, Portfolio value: 10,600 USD, Trade PnL: +100 USD ...and so on. Additionally, an end-of-day summary can be logged with total PnL and Sharpe.
* Dashboard (Optional): We can create a simple dashboard to visualize the bot’s performance in real-time. Using Streamlit or Plotly Dash, we can display:
    * An updating line chart of the portfolio value vs. time, to see equity growth.
    * A bar or gauge for current PnL.
    * The current Sharpe Ratio (perhaps computed on the fly using the series of returns so far).
    * A table of recent trades (with timestamp, action, price, PnL per trade).
    * Indicators like current position and available balance.
* For example, a Streamlit app could refresh these metrics every few seconds by reading from the log or by directly accessing the running bot’s variables (if integrated). This gives a nice visual feedback loop. It’s optional but highly useful for monitoring and for demo purposes. Plotly can also be used to generate interactive charts of price with buy/sell markers – e.g., plotting the market price and placing green upward arrows for buys and red downward arrows for sells (as was done in some trading RL visualizations).
All logs and metrics not only help in monitoring but also allow calculating the 24-hour portfolio return (simply (final_portfolio_value - starting_value)/starting_value * 100%) and comparing it to benchmarks. We schedule the bot to run in 24h sessions, after which it can output the summary. If we were to run continuously beyond 24h, we could still segment performance by day.
By analyzing the logs, we can refine both the model and the risk management. For instance, if we notice many small trades around noise, we might raise the action threshold. If the Sharpe ratio is lower than expected due to a few large losses, we might tighten stop-loss or reduce position size. This feedback loop ensures the system keeps improving.
8. Project Structure and Deployment
We organize the project into a clear folder structure for maintainability:
crypto-rl-bot/
├── data/
│   └── historical_data.csv        # e.g., CSV of price history used for training
├── env/
│   └── trading_env.py             # Custom Gym environment definition (TradingEnv class)
├── train.py                       # Training script using Stable Baselines3 (trains the RL model)
├── models/
│   └── trading_bot_ppo.zip        # Saved model weights after training
├── trading_bot.py                 # Live trading script: connects to Roostoo API and runs the loop
├── utils.py                       # Utility functions (API signing, data preprocessing, etc.)
├── config.py                      # Configuration (API keys, secret, and configurable params)
├── logs/
│   └── trades.log                 # Log file for trades and events
└── dashboard.py                   # (Optional) Streamlit app for monitoring
Data: Contains historical price data. This can be a CSV from Kaggle or data pulled from Binance. We might have a script to fetch and update this data. The historical data is used by trading_env.py for simulating the market during training.
Environment (trading_env.py): Defines the TradingEnv class inheriting from gym.Env. This implements __init__, reset, step, etc., as discussed. It reads the historical data, normalizes it, and simulates the trading. We ensure it’s compatible with Stable Baselines3 (observation space and action space defined). For example, reset() will set initial balance and start index, and return the initial observation. step(action) will apply the action to update balances and move to the next price, calculate reward, and return (obs, reward, done, info).
Training Script (train.py): Sets up the environment and agent for training. It loads the data, creates the TradingEnv, then initializes the PPO model from Stable Baselines3. It then calls model.learn() for a certain number of timesteps. We might include callbacks for saving the model periodically or evaluating on a validation set. After training, it saves the model weights to the models/ directory. This script can be run offline (not connected to Roostoo) since it uses stored data.
Live Trading Bot (trading_bot.py): This is the main runtime script for live deployment. It loads the trained model (e.g. using PPO.load() to get the policy). It also loads API keys from config.py. The script then enters a loop as described in Section 5:
* Retrieve price from Roostoo /v3/ticker (using a helper in utils.py perhaps).
* Formulate observation and get action from model.
* Check thresholds, risk conditions, etc.
* If decided, place order with /v3/place_order using signed request (using utils.py signing function to generate MSG-SIGNATURE (GitHub - roostoo/Roostoo-API-Documents) (GitHub - roostoo/Roostoo-API-Documents)).
* Log the outcome, update state.
* Sleep for a specified interval ( or wait for next tick). The script should handle termination signals to exit cleanly (perhaps cancel open orders or just safely break the loop). It will continuously append to the logs/trades.log and could also print updates to console for quick monitoring.
Utils (utils.py): Contains supporting functions such as:
* get_ticker(pair): GET request to Roostoo ticker API (as in the provided PDF code) to fetch latest price.
* sign_request(params): generate HMAC SHA256 signature given a params dict and secret key (GitHub - roostoo/Roostoo-API-Documents).
* Possibly formatting functions for logs or technical indicator calculations if needed (though indicators might be computed inside the environment or model input pipeline).
* Anything reused in multiple places (like timestamp generation in ms).
Config (config.py): Stores configuration constants and sensitive info:
* API_KEY and SECRET_KEY for Roostoo.
* Trading parameters like default TRADE_QUANTITY or STOP_LOSS_PCT, etc., so they can be tweaked easily.
* Possibly toggles for simulation mode vs live mode. We keep this separate so that we don’t accidentally share secrets (e.g., we can add this file to .gitignore). Security is important – the secret key should be kept private and not logged. All API calls use the key from here.
Logs: Contains trades.log which will accumulate logs. We might also save a CSV of trade history here for analysis. In addition, if using TensorBoard for monitoring training, those logs would be in a tensorboard/ directory (as configured in training script).
Dashboard (dashboard.py): If implemented, this could use Streamlit. It would read the log or perhaps connect to the running bot via a socket or shared file. Simpler is reading the log periodically:
* Use pandas to read trades.log or a CSV of portfolio value over time.
* Compute latest metrics (PnL, Sharpe).
* Plot the charts (Streamlit has line chart components or one can use Plotly and display).
* This script would be run separately (streamlit run dashboard.py) while the bot is running.
Running the Bot: To run the system, we would:
1. Ensure config.py has the API credentials and desired settings.
2. (Optional) Run train.py if we don’t have a trained model yet or want to retrain on new data.
3. Start the live trading: python trading_bot.py. This will load the model and begin trading. We monitor the console or log file for activity.
4. (Optional) Start the dashboard: e.g. streamlit run dashboard.py to visualize performance in real-time.
Throughout development, we can test the pipeline on the Roostoo sandbox by perhaps limiting to very small trade sizes or even a paper-trading mode (Roostoo mock exchange likely doesn’t use real funds anyway). This ensures the logic is correct before any real money deployment (if one were to adapt the code to a real exchange).
Finally, the project would include a README explaining how to install requirements (e.g. pip install stable-baselines3 streamlit requests numpy pandas etc.), how to configure keys, and how to run training and trading. All code is in Python for consistency. By following this setup, one should be able to reproduce the training and then launch the autonomous trading bot on the Roostoo mock exchange, achieving (hopefully) optimized 24h returns and a high Sharpe ratio through the combination of deep RL decisions and prudent risk management.
